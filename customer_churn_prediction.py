# -*- coding: utf-8 -*-
"""Customer churn prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FiVdRRdK09TN58trEZk1sYkLtnBoDy7M
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install scikeras

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model
from keras import layers
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.preprocessing import StandardScaler
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score
from tensorflow.keras.layers import Input, Dense
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import plotly.express as px

"""**Data Handling and Cleaning**"""

df = pd.read_csv('/content/drive/My Drive/Mygoogle Folder/CustomerChurn_dataset.csv')

df.head()

threshold = 0.3 * len(df)
df.dropna(thresh=threshold, axis=1, inplace=True)

nan_col = df.isnull().sum()
nan_col

nan_col.dtypes

df[nan_col.index].dtypes

# Select columns from 'df' based on the indices from 'nan_col' and filter by non-numeric data types
alpha_df = df[nan_col.index].select_dtypes(exclude=['number'])

# Count the number of missing values in 'alpha_df'
alpha_missing_values = alpha_df.isnull().sum()
alpha_missing_values

# Define the columns to convert to integers and floats
int_columns = ["SeniorCitizen", "tenure"]
float_columns = ["MonthlyCharges", "TotalCharges"]

# Convert specified columns to integers
df[int_columns] = df[int_columns].astype(int)

# Replace empty strings with NaN in float columns
df[float_columns] = df[float_columns].replace(' ', np.nan)

# Convert the float columns, handling non-convertible values by coercing to NaN
for col in float_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Display the data types after conversion
converted_dtypes = df.dtypes
print(converted_dtypes)

# Define columns for integer and float data type conversion
int_conversion_columns = ["SeniorCitizen", "tenure"]
float_conversion_columns = ["MonthlyCharges", "TotalCharges"]

# Convert specified columns to integer data type
df[int_conversion_columns] = df[int_conversion_columns].astype(int)

# Replace empty strings with NaN in float columns, handling missing data
df[float_conversion_columns] = df[float_conversion_columns].replace(' ', np.nan)

# Convert float columns to numeric values, handling non-convertible values by coercing to NaN
for float_col in float_conversion_columns:
    df[float_col] = pd.to_numeric(df[float_col], errors='coerce')

# Display the resulting data types after conversion
converted_data_types = df.dtypes
print(converted_data_types)

# Fill missing values in the DataFrame with the median of each column
df = df.fillna(df.median())

# Extract columns from 'df' based on the indices in 'NullData' and filter for numeric data types
selected_numeric_columns = df[nan_col.index].select_dtypes(include='number')

# Count missing values in the selected numeric columns
missing_values_count = selected_numeric_columns.isnull().sum()

# Display the selected numeric columns and their missing value counts
print(selected_numeric_columns)
print("Missing Value Counts:")
print(missing_values_count)

"""# Data Visualization"""

def distplot(feature, frame, color='r'):
    plt.figure(figsize=(8,3))
    plt.title("Distribution for {}".format(feature))
    ax = sns.distplot(frame[feature], color= color)
num_cols = ["tenure", 'MonthlyCharges', 'TotalCharges']
for feat in num_cols: distplot(feat, df)

msno.matrix(df);

fig = px.histogram(df, x="Churn", color="Contract", barmode="group", title="<b>Customer contract distribution<b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

plt.figure(figsize=(6, 6))
labels =["Churn: Yes","Churn:No"]
values = [1869,5163]
labels_gender = ["F","M","F","M"]
sizes_gender = [939,930 , 2544,2619]
colors = ['#ff6666', '#66b3ff']
colors_gender = ['#c2c2f0','#ffb3e6', '#c2c2f0','#ffb3e6']
explode = (0.3,0.3)
explode_gender = (0.1,0.1,0.1,0.1)
textprops = {"fontsize":15}
#Plot
plt.pie(values, labels=labels,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )
plt.pie(sizes_gender,labels=labels_gender,colors=colors_gender,startangle=90, explode=explode_gender,radius=7, textprops =textprops, counterclock = True, )
#Draw circle
centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title('Churn Distribution w.r.t Gender: Male(M), Female(F)', fontsize=15, y=1.1)

# show plot

plt.axis('equal')
plt.tight_layout()
plt.show()

g_labels = ['Male', 'Female']
c_labels = ['No', 'Yes']
# Create subplots: use 'domain' type for Pie subplot
fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])
fig.add_trace(go.Pie(labels=g_labels, values=df['gender'].value_counts(), name="Gender"),
              1, 1)
fig.add_trace(go.Pie(labels=c_labels, values=df['Churn'].value_counts(), name="Churn"),
              1, 2)

# Use `hole` to create a donut-like pie chart
fig.update_traces(hole=.4, hoverinfo="label+percent+name", textfont_size=16)

fig.update_layout(
    title_text="Gender and Churn Distributions",
    # Add annotations in the center of the donut pies.
    annotations=[dict(text='Gender', x=0.16, y=0.5, font_size=20, showarrow=False),
                 dict(text='Churn', x=0.84, y=0.5, font_size=20, showarrow=False)])
fig.show()

color_map = {"Yes": "#FF97FF", "No": "#AB63FA"}
fig = px.histogram(df, x="Churn", color="Dependents", barmode="group", title="<b>Dependents distribution</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

color_map = {"Yes": '#FFA15A', "No": '#00CC96'}
fig = px.histogram(df, x="Churn", color="Partner", barmode="group", title="<b>Chrun distribution w.r.t. Partners</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

fig = px.box(df, x='Churn', y = 'tenure')

# Update yaxis properties
fig.update_yaxes(title_text='Tenure (Months)', row=1, col=1)
# Update xaxis properties
fig.update_xaxes(title_text='Churn', row=1, col=1)

# Update size and title
fig.update_layout(autosize=True, width=750, height=600,
    title_font=dict(size=25, family='Courier'),
    title='<b>Tenure vs Churn</b>',
)

fig.show()

fig = go.Figure()

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [965, 992, 219, 240],
  name = 'DSL',
))

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [889, 910, 664, 633],
  name = 'Fiber optic',
))

fig.add_trace(go.Bar(
  x = [['Churn:No', 'Churn:No', 'Churn:Yes', 'Churn:Yes'],
       ["Female", "Male", "Female", "Male"]],
  y = [690, 717, 56, 57],
  name = 'No Internet',
))

fig.update_layout(title_text="<b>Churn Distribution w.r.t. Internet Service and Gender</b>")

fig.show()

fig = px.histogram(df, x="Churn", color="TechSupport",barmode="group",  title="<b>Chrun distribution w.r.t. TechSupport</b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

sns.set_context("paper",font_scale=1.1)
ax = sns.kdeplot(df.MonthlyCharges[(df["Churn"] == 'No') ],
                color="Red", shade = True);
ax = sns.kdeplot(df.MonthlyCharges[(df["Churn"] == 'Yes') ],
                ax =ax, color="Blue", shade= True);
ax.legend(["Not Churn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Monthly Charges');
ax.set_title('Distribution of monthly charges by churn');

ax = sns.kdeplot(df.TotalCharges[(df["Churn"] == 'No') ],
                color="Gold", shade = True);
ax = sns.kdeplot(df.TotalCharges[(df["Churn"] == 'Yes') ],
                ax =ax, color="Green", shade= True);
ax.legend(["Not Chu0rn","Churn"],loc='upper right');
ax.set_ylabel('Density');
ax.set_xlabel('Total Charges');
ax.set_title('Distribution of total charges by churn');

pip install plotly

labels = df['PaymentMethod'].unique()
values = df['PaymentMethod'].value_counts()

fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])
fig.update_layout(title_text="<b>Payment Method Distribution</b>")
fig.show()

plt.figure(figsize=(25, 10))

corr = df.apply(lambda x: pd.factorize(x)[0]).corr()

mask = np.triu(np.ones_like(corr, dtype=bool))

ax = sns.heatmap(corr, mask=mask, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, linewidths=.2, cmap='coolwarm', vmin=-1, vmax=1)

"""**Scaling and Label Encoding**"""

# Import the StandardScaler from the scikit-learn libraryfrom sklearn.preprocessing import StandardScaler

# Create a StandardScaler instance
data_scaler = StandardScaler()

# Create a copy of the numeric DataFrame for standardized data
standardized_data = selected_numeric_columns.copy()

# Use the StandardScaler to standardize the numeric columns
standardized_data[selected_numeric_columns.columns] = data_scaler.fit_transform(selected_numeric_columns[selected_numeric_columns.columns])

# Create a new DataFrame containing the standardized numeric data
numeric_scaled_df = standardized_data

# Display the DataFrame with the standardized numeric data
print(numeric_scaled_df)

# Remove the 'customerId' column from the DataFrame
df = df.drop('customerID', axis=1)

# Remove numerical columns specified in 'num_col' from the DataFrame
categorical_columns = df.drop(selected_numeric_columns, axis=1)

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Iterate through each column in the DataFrame
for column in categorical_columns.columns:
    # Check if the column contains categorical data
    if categorical_columns[column].dtype == 'object':
        # Apply label encoding to the categorical column
        categorical_columns[column] = label_encoder.fit_transform(categorical_columns[column])

# Combine the encoded categorical dataset with the original numerical columns
combined_df = pd.concat([categorical_columns, selected_numeric_columns], axis=1)
combined_df

"""**Performing feature importance and training the model**"""

# Assuming 'target' is your target variable (e.g., 'Churn' column)
X = combined_df.drop('Churn', axis=1)
y = combined_df['Churn']

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier()

# Fit the classifier to your data
rf_classifier.fit(X, y)

# Get feature importances
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame to store feature names and their importances
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
top_features = 15  # Change this value to plot a different number of top features
plt.figure(figsize=(12, 6))
plt.barh(importance_df['Feature'][:top_features], importance_df['Importance'][:top_features])
plt.xlabel('Feature Importance')
plt.title('Feature Importance')
plt.show()
relevant_features=importance_df['Feature'][:top_features].values

X=combined_df[relevant_features]

# Split relevant features into training and testing sets
X_train,Xcb_dataset,y_train,ycb_dataset = train_test_split(X,y, test_size=0.2, random_state=42)

X_validation,X_test,y_validation,y_test = train_test_split(Xcb_dataset,ycb_dataset, test_size=0.2, random_state=42)

# Assuming X_train, y_train, X_validation, y_validation are available

# Input layer
input_layer = Input(shape=(X_train.shape[1],))

# Dense layers
dense_layer_1 = Dense(64, activation='relu')(input_layer)
dense_layer_2 = Dense(32, activation='relu')(dense_layer_1)

# Output layer
output_layer = Dense(1, activation='sigmoid')(dense_layer_2)

# Define the model
model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(X_validation, y_validation),
    verbose=0
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')

!pip install scikeras

# Function to create the Keras model
def create_model(units=64):
    input_layer = Input(shape=(X_train.shape[1],))
    dense_layer_1 = Dense(units, activation='relu')(input_layer)
    dense_layer_2 = Dense(32, activation='relu')(dense_layer_1)
    output_layer = Dense(1, activation='sigmoid')(dense_layer_2)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Create a KerasClassifier from Sci-Keras
keras_model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=True)

# Define hyperparameters for grid search
param_grid = {
    'batch_size': [16, 32, 64],
    'optimizer': ['adam', 'sgd', 'rmsprop'],
}

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=keras_model, param_grid=param_grid, scoring=['accuracy', 'roc_auc'], refit='roc_auc', cv=3, verbose=True)
grid_result = grid_search.fit(X_train, y_train, validation_data=(X_validation, y_validation),verbose=True)

# Get the best parameters
best_params = grid_result.best_params_
print("Best Parameters:", best_params)

# Get the best model
best_model = grid_result.best_estimator_
best_model_main = grid_result.best_estimator_.model

# Evaluate the best model on the test set
y_pred = best_model.predict(X_test)
test_auc = roc_auc_score(y_test, y_pred)
test_accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))

print(f'Test AUC: {test_auc}, Test Accuracy: {test_accuracy}')

from sklearn.metrics import roc_auc_score, accuracy_score

# Evaluate the best model on the test set
test_loss = best_model.score(X_test, y_test)

# Alternatively, you can use best_model.predict_proba for obtaining probability estimates
y_pred_proba = best_model.predict_proba(X_test)[:, 1]
test_auc = roc_auc_score(y_test, y_pred_proba)

print(f'Test Loss: {test_loss}, Test AUC: {test_auc}')

best_model=grid_result.best_estimator_

mk_prediciton=best_model.predict(X_test)

from sklearn.metrics import roc_auc_score, accuracy_score

# Set best_model to the best_estimator_
best_model = grid_result.best_estimator_

# Evaluate the best model on the test set
test_loss = best_model.score(X_test, y_test)

# Get predictions using predict method
y_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))

print(f'Test Loss: {test_loss}, Test AUC: {test_auc}, Test Accuracy: {test_accuracy}')

# Retrain the model
best_model.fit(X_train, y_train,epochs=10, batch_size=16, verbose=True)

# Set best_model to the best_estimator_
best_model = grid_result.best_estimator_

# Evaluate the best model on the test set
test_loss = best_model.score(X_test, y_test)

# Get predictions using predict method
y_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))

# Calculate AUC score
y_pred_proba = best_model.predict_proba(X_test)[:, 1]
test_auc = roc_auc_score(y_test, y_pred_proba)

print(f'Test Loss: {test_loss}, Test AUC: {test_auc}, Test Accuracy: {test_accuracy}')

# Retrain the model
best_model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=True)

# Evaluate the model again
retest_loss = best_model.score(X_test, y_test)
y_pred_retest = best_model.predict(X_test)
retest_accuracy = accuracy_score(y_test, (y_pred_retest > 0.5).astype(int))
y_pred_proba_retest = best_model.predict_proba(X_test)[:, 1]
retest_auc = roc_auc_score(y_test, y_pred_proba_retest)

print(f'Retested Loss: {retest_loss}, Retested AUC: {retest_auc}, Retested Accuracy: {retest_accuracy}')

"""**Saving the retrained model**"""

## Save the model
model = model.save('retrained_model.h5')

## load the saved model to use for prediction
model = load_model('retrained_model.h5')